{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlite3\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "import string\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.feature_extraction.text import TfidfTransformer,TfidfVectorizer,CountVectorizer\n",
    "from sklearn.metrics import confusion_matrix , roc_curve , auc\n",
    "from nltk.stem.porter import PorterStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "con = sqlite3.connect('/home/arnab/Desktop/DATASETS/AmazonFineFoodReviews/database.sqlite')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Filtering only positive and negative reviews:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_data = pd.read_sql_query(\"\"\"SELECT*FROM Reviews WHERE Score!=3\"\"\",con)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def partition(x):\n",
    "    if x<3:\n",
    "        return 'negative'\n",
    "    return 'positive'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "actualScore = filtered_data['Score']\n",
    "posNeg = actualScore.map(partition)\n",
    "filtered_data['Score'] = posNeg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(525814, 10)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>ProductId</th>\n",
       "      <th>UserId</th>\n",
       "      <th>ProfileName</th>\n",
       "      <th>HelpfulnessNumerator</th>\n",
       "      <th>HelpfulnessDenominator</th>\n",
       "      <th>Score</th>\n",
       "      <th>Time</th>\n",
       "      <th>Summary</th>\n",
       "      <th>Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>B001E4KFG0</td>\n",
       "      <td>A3SGXH7AUHU8GW</td>\n",
       "      <td>delmartian</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>positive</td>\n",
       "      <td>1303862400</td>\n",
       "      <td>Good Quality Dog Food</td>\n",
       "      <td>I have bought several of the Vitality canned d...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>B00813GRG4</td>\n",
       "      <td>A1D87F6ZCVE5NK</td>\n",
       "      <td>dll pa</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>negative</td>\n",
       "      <td>1346976000</td>\n",
       "      <td>Not as Advertised</td>\n",
       "      <td>Product arrived labeled as Jumbo Salted Peanut...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>B000LQOCH0</td>\n",
       "      <td>ABXLMWJIXXAIN</td>\n",
       "      <td>Natalia Corres \"Natalia Corres\"</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>positive</td>\n",
       "      <td>1219017600</td>\n",
       "      <td>\"Delight\" says it all</td>\n",
       "      <td>This is a confection that has been around a fe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>B000UA0QIQ</td>\n",
       "      <td>A395BORC6FGVXV</td>\n",
       "      <td>Karl</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>negative</td>\n",
       "      <td>1307923200</td>\n",
       "      <td>Cough Medicine</td>\n",
       "      <td>If you are looking for the secret ingredient i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>B006K2ZZ7K</td>\n",
       "      <td>A1UQRSCLF8GW1T</td>\n",
       "      <td>Michael D. Bigham \"M. Wassir\"</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>positive</td>\n",
       "      <td>1350777600</td>\n",
       "      <td>Great taffy</td>\n",
       "      <td>Great taffy at a great price.  There was a wid...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Id   ProductId          UserId                      ProfileName  \\\n",
       "0   1  B001E4KFG0  A3SGXH7AUHU8GW                       delmartian   \n",
       "1   2  B00813GRG4  A1D87F6ZCVE5NK                           dll pa   \n",
       "2   3  B000LQOCH0   ABXLMWJIXXAIN  Natalia Corres \"Natalia Corres\"   \n",
       "3   4  B000UA0QIQ  A395BORC6FGVXV                             Karl   \n",
       "4   5  B006K2ZZ7K  A1UQRSCLF8GW1T    Michael D. Bigham \"M. Wassir\"   \n",
       "\n",
       "   HelpfulnessNumerator  HelpfulnessDenominator     Score        Time  \\\n",
       "0                     1                       1  positive  1303862400   \n",
       "1                     0                       0  negative  1346976000   \n",
       "2                     1                       1  positive  1219017600   \n",
       "3                     3                       3  negative  1307923200   \n",
       "4                     0                       0  positive  1350777600   \n",
       "\n",
       "                 Summary                                               Text  \n",
       "0  Good Quality Dog Food  I have bought several of the Vitality canned d...  \n",
       "1      Not as Advertised  Product arrived labeled as Jumbo Salted Peanut...  \n",
       "2  \"Delight\" says it all  This is a confection that has been around a fe...  \n",
       "3         Cough Medicine  If you are looking for the secret ingredient i...  \n",
       "4            Great taffy  Great taffy at a great price.  There was a wid...  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(filtered_data.shape)\n",
    "filtered_data.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Cleaning the data:***"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Removing Duplicates:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_data = filtered_data.sort_values('ProductId',axis=0,ascending=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(364173, 10)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final= sorted_data.drop_duplicates(subset={\"UserId\",\"ProfileName\",\"Time\",\"Text\"},keep=\"first\",inplace=False)\n",
    "final.shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "HelpfulnessDenominator>=HelpfulnessNumerator:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "final=final[final.HelpfulnessDenominator>=final.HelpfulnessNumerator]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(364171, 10)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "positive    307061\n",
       "negative     57110\n",
       "Name: Score, dtype: int64"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(final.shape)\n",
    "final['Score'].value_counts()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***BAG OF WORDS:***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_vect = CountVectorizer()\n",
    "final_counts = count_vect.fit_transform(final['Text'].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'scipy.sparse._csr.csr_matrix'>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(364171, 115281)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(type(final_counts))\n",
    "final_counts.get_shape()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Text Preprocessing:***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'amoungst', 'they', 'are', 'is', 'done', 'call', 'less', 'where', 'much', 'everything', 'not', 'get', 'hundred', 'to', 'therein', 'been', 'whereby', 'keep', 'ten', 'by', 'within', 'further', 'almost', 'hence', 'why', 'due', 'nevertheless', 'whether', 'fifteen', 'whereupon', 'thus', 'anyhow', 'your', 'there', 'front', 'back', 'thin', 'bottom', 'towards', 'should', 'seeming', 'anyway', 'whom', 'upon', 'former', 'afterwards', 'sincere', 'below', 'off', 'all', 'whatever', 'itself', 'seems', 'beside', 'else', 'mill', 'for', 'please', 'five', 'its', 'each', 'out', 'being', 'might', 'those', 'him', 'next', 'nothing', 'name', 'onto', 'themselves', 'be', 'full', 'meanwhile', 'together', 'do', 'thick', 'their', 'wherever', 'his', 'until', 'hers', 'always', 'mostly', 'it', 'cry', 'rather', 'third', 'something', 'so', 'once', 'everywhere', 'found', 'up', 'fire', 'take', 'have', 'part', 'fifty', 'three', 'two', 'cannot', 'he', 'was', 'still', 'at', 'along', 'throughout', 'also', 'bill', 'becoming', 'noone', 'whence', 'over', 'during', 'these', 'nine', 'often', 'than', 'made', 'everyone', 'above', 'became', 'somewhere', 'hereby', 'hasnt', 'herein', 'such', 'the', 'a', 'me', 'across', 'in', 'other', 'ours', 'already', 'neither', 'among', 'then', 'seemed', 'more', 'co', 'mine', 'give', 'any', 'and', 'see', 'namely', 'first', 'moreover', 'otherwise', 'whose', 'before', 'of', 'system', 'though', 'last', 'interest', 'becomes', 'ever', 'between', 'am', 'top', 'yet', 'whereas', 'i', 'sometime', 'etc', 'nor', 'were', 'myself', 'behind', 'again', 'nobody', 'both', 'around', 'her', 'de', 'who', 'many', 'thereby', 'well', 'anything', 'wherein', 'amongst', 'has', 'our', 'about', 'or', 'therefore', 'too', 'thereupon', 'which', 'when', 'thence', 'except', 'show', 'thereafter', 'after', 'anyone', 'via', 'perhaps', 'whereafter', 'she', 'under', 'herself', 'several', 'eg', 'whenever', 'alone', 'amount', 'without', 'go', 'an', 'same', 'however', 'own', 'although', 'twelve', 'here', 'must', 'eleven', 'from', 'whoever', 'ourselves', 'yourself', 'into', 'latterly', 'my', 'as', 'beforehand', 'can', 'because', 'find', 'couldnt', 'six', 'none', 'may', 'formerly', 'we', 'whither', 'you', 'few', 'con', 'cant', 'sometimes', 'if', 'toward', 'per', 'ie', 'four', 'one', 'yours', 'now', 'even', 'inc', 'no', 'put', 'latter', 'himself', 'eight', 'us', 'seem', 'every', 'describe', 'enough', 'move', 'detail', 'twenty', 'with', 'beyond', 'ltd', 'that', 'un', 'indeed', 'nowhere', 'would', 'some', 'could', 'hereupon', 're', 'down', 'yourselves', 'forty', 'against', 'whole', 'others', 'on', 'empty', 'fill', 'had', 'hereafter', 'while', 'never', 'side', 'them', 'either', 'will', 'only', 'through', 'most', 'another', 'thru', 'sixty', 'somehow', 'very', 'least', 'anywhere', 'become', 'what', 'besides', 'this', 'elsewhere', 'since', 'how', 'someone', 'serious', 'but'}\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import nltk\n",
    "from sklearn.feature_extraction.text import ENGLISH_STOP_WORDS\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "\n",
    "stop = set(ENGLISH_STOP_WORDS)\n",
    "sno = nltk.stem.SnowballStemmer('english')\n",
    "\n",
    "def cleanhtml(sentence):\n",
    "    cleanr = re.compile('<.*?>')\n",
    "    cleantext=re.sub(cleanr,' ',sentence)\n",
    "    return cleantext\n",
    "def cleanpunc(sentence):\n",
    "    cleaned=re.sub(r'[?|!|\\'|\"|#]',r' ',sentence)\n",
    "    cleaned=re.sub(r'[.|,|)|(|\\|/]',r' ',cleaned)\n",
    "    return cleaned\n",
    "\n",
    "print(stop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "i=0\n",
    "strl=' '\n",
    "final_string=[]\n",
    "all_pos_words=[]\n",
    "all_neg_words=[]\n",
    "s=''\n",
    "for sent in final['Text'].values:\n",
    "    filtered_sent=[]\n",
    "    sent=cleanhtml(sent)\n",
    "    for w in sent.split():\n",
    "        for cleaned_words in cleanpunc(w).split():\n",
    "            if((cleaned_words.isalpha())&(len(cleaned_words)>2)):\n",
    "                if(cleaned_words.lower() not in stop):\n",
    "                    s=(sno.stem(cleaned_words.lower())).encode('utf8')\n",
    "                    filtered_sent.append(s)\n",
    "                    if(final['Score'].values)[i] == 'positive':\n",
    "                        all_pos_words.append(s)\n",
    "                    if(final['Score'].values)[i] == 'negative': \n",
    "                        all_neg_words.append(s)\n",
    "                else:\n",
    "                    continue\n",
    "            else:\n",
    "                continue\n",
    "    strl =b\" \".join(filtered_sent)\n",
    "    final_string.append(strl)\n",
    "    i+=1                                    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "138706    b'witti littl book make son laugh loud recit c...\n",
       "138688    b'grew read sendak book watch realli rosi movi...\n",
       "138689    b'fun way children learn month year learn poem...\n",
       "138690    b'great littl book read nice rhythm good repet...\n",
       "138691    b'book poetri month year goe month cute littl ...\n",
       "Name: CleanedText, dtype: object"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final['CleanedText']=final_string\n",
    "final['CleanedText'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "364171"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conn = sqlite3.connect('final.sqlite')\n",
    "c=conn.cursor()\n",
    "conn.text_factory=str\n",
    "final.to_sql('Reviews',conn,schema=None,if_exists='replace')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***BI-GRAMS and n-GRAMS:***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(b'like', 139432), (b'tast', 129068), (b'good', 112789), (b'flavor', 109637), (b'love', 107396), (b'great', 103924), (b'use', 103893), (b'product', 91054), (b'just', 88378), (b'tri', 86803)]\n",
      "[(b'tast', 34592), (b'like', 32331), (b'product', 28227), (b'flavor', 19580), (b'just', 18763), (b'tri', 17756), (b'use', 15303), (b'good', 15044), (b'coffe', 14720), (b'buy', 13752)]\n"
     ]
    }
   ],
   "source": [
    "freq_dist_pos=nltk.FreqDist(all_pos_words)\n",
    "freq_dist_neg=nltk.FreqDist(all_neg_words)\n",
    "print(freq_dist_pos.most_common(10))\n",
    "print(freq_dist_neg.most_common(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_vect=CountVectorizer(ngram_range=(1,2))\n",
    "final_bigrams_counts=count_vect.fit_transform(final['Text'].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(364171, 2910192)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_bigrams_counts.get_shape()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***TF-IDF:***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_idf_vect=TfidfVectorizer(ngram_range=(1,2))\n",
    "final_tf_idf=tf_idf_vect.fit_transform(final['Text'].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(364171, 2910192)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_tf_idf.get_shape()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2910192"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features = tf_idf_vect.get_feature_names_out()\n",
    "len(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['ales until', 'ales ve', 'ales would', 'ales you', 'alessandra',\n",
       "       'alessandra ambrosia', 'alessi', 'alessi added', 'alessi also',\n",
       "       'alessi and'], dtype=object)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features[100000:100010]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0. 0. 0. ... 0. 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "print((final_tf_idf[3,:]).toarray()[0])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"***IMP\"Getting Top Features:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def top_tf_idf_feats(rows,features,n=25):\n",
    "    topn_ids=np.argsort(rows)[::-1][:n]\n",
    "    top_feats=[(features[i],rows[i])for i in topn_ids]\n",
    "    df=pd.DataFrame(top_feats)\n",
    "    df.columns=['feature','tfidf']\n",
    "    return df\n",
    "top_tfidf=top_tf_idf_feats(final_tf_idf[1,:].toarray()[0],features,25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>feature</th>\n",
       "      <th>tfidf</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>sendak books</td>\n",
       "      <td>0.173437</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>rosie movie</td>\n",
       "      <td>0.173437</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>paperbacks seem</td>\n",
       "      <td>0.173437</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>cover version</td>\n",
       "      <td>0.173437</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>these sendak</td>\n",
       "      <td>0.173437</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           feature     tfidf\n",
       "0     sendak books  0.173437\n",
       "1      rosie movie  0.173437\n",
       "2  paperbacks seem  0.173437\n",
       "3    cover version  0.173437\n",
       "4     these sendak  0.173437"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top_tfidf.head()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
